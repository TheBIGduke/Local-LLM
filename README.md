# Local LLM for Robots ü§ñü¶æ

[![Python](https://img.shields.io/badge/Python-3.10+-yellow.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)


**Local-LLM-for-Robots** is a python package that fuses a local Large Language Model with a full offline speech pipeline, so robots can understand and respond using natural language without the cloud. This repo includes everything about the `Wake Word (activation word)` -> `STT (speech-to-text)` -> `LLM Module` -> `TTS (text-to-speech)`. Every module is organized in different folders. Therefore, if you want more details about the `LLM Module`, you can check the `README.md` that is into the `/llm` folder.

---
## üìù Flowchart
<figure style="margin:0; text-align:center; border:1px solid #eaecef; padding:6px; border-radius:6px;">
  <img src="docs/Local-LLM-for-Robots.svg"
       alt="Flow Chart"
       style="max-width:100%; height:auto;" />
  <figcaption style="font-size:0.9em; color:#6a737d; margin-top:4px;">
    View the full Pipeline
  </figcaption>
</figure>
<p align="right">
  <a href="https://lucid.app/lucidchart/50ed3019-62f3-460d-a3e3-071d72727e35/view">Open in real size</a>
</p>

---
## üé• Short Demo
Here‚Äôs a **short demo of the Avatar system in action** ‚Äî showing how the visualization reacts when saying the wake-word and triggers the full interaction pipeline.

<p align="center">
  <a href="https://youtu.be/PP4M3LmFDbM" target="_blank">
    <img src="https://img.youtube.com/vi/PP4M3LmFDbM/hqdefault.jpg" width="720" alt="Avatar Demo YouTube">
  </a>
</p>

When the **wake word is detected**, the avatar changes its color and responds with speech.
> [!NOTE]
> This avatar system was originally developed by [TheBIGduke](https://github.com/TheBIGduke/OctoV) 

![Avatar Demo](docs/avatar/avatar.gif)

---


## üìö Table of Contents
- [Installation](#installation)
- [Configuration](#configuration)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Contributing](#contributing) 
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

<h2 id="installation">üõ†Ô∏è Installation</h2>

> [!IMPORTANT]
> This implementation was tested on Ubuntu 22.04 with Python 3.10.12 

### Prerequisites
- Git, CMake
- (Optional) NVIDIA CUDA for GPU acceleration

### Cloning this Repo
```bash
# Clone the repository
git clone https://github.com/JossueE/Local-LLM-for-Robots.git 
cd Local-LLM-for-Robots
```
### Setup

#### For automatic installation and setup, run the installer:
```bash
bash installer.sh
```

#### For manual installation and setup:

```bash
sudo apt update

# --- General installations ---
sudo apt install -y python3-dev python3-venv build-essential curl unzip

# --- STT (Speech-to-Text) ---
sudo apt install -y portaudio19-dev ffmpeg

# --- TTS (Text-to-Speech) ---
# ffmpeg is already installed above, uncomment if you prefer to keep it separate
# sudo apt install -y ffmpeg

# --- LLM (YAML manipulation) ---
sudo snap install yq
```
```bash
# Create a virtual environment
python3 -m venv .venv
source .venv/bin/activate
```
```bash
# Install dependencies
pip install -r requirements.txt
```
To check if the models were correctly downloaded or to download models:
```bash
bash utils/download_models.sh
```
The script installs everything into your cache directory (`~/.cache/Local-LLM-for-Robots`, or `$OCTOPY_CACHE` if set).
You‚Äôre done when you see:
```bash
"OK. Modelos listos en: $CACHE_DIR ‚úÖ "
```
---

<h2 id="configuration">‚öôÔ∏è Configuration</h2>

> [!WARNING] 
> LLMs and audio models can be large. Ensure you have enough **disk space** and **RAM/VRAM** for your chosen settings.

### üîß General Settings (`config/settings.py`)
All runtime settings live in **`config/settings.py`**. They are plain Python constants‚Äîedit the file and restart your scripts to apply changes.

### üì¶ Model catalog (`config/models.yml`)
Define which models Octybot uses (LLM, STT, TTS, wake-word) along with their URLs and sample rates.

### üîó Sytem Prompt Definition with `config/llm_system_prompt_def.py`
To re-write or define a new **LLM - System Prompt**  

### üåé Data for Commun Questions and Places in a Map
All general questions live in `config/general_rag.json`. Define the **New Question** in `triggers` and define the `answer`.

<h2 id="quick-start">‚ö° Quick Start</h2>

```bash
cd Local-LLM-for-Robots 
source .venv/bin/activate
```

### Launch the full pipeline (Wake-Word ‚Üí STT ‚Üí LLM ‚Üí TTS)

Start everything with:
```bash
python -m main
```
Now say `ok robot` ‚Äî the system will start listening and run the pipeline.

### Run a Single Module‚Äôs Tests

LLM Module
```bash
python -m llm.llm
```

Speech to Text Module
```bash
#To test the Audio Listener 
python -m stt.AudioListener

#To test the Wake Word Detector
python -m stt.wake_word

#To test the Speech To Text
python -m stt.speech_to_text
```

Text to Speech Module
```bash
python -m tts.text_to_speech
```

> [!TIP]
> If you have some problems to launch modules, you should try to run with the `venv` as `./.venv/bin/python -m stt.speech_to_text`

<h2 id="usage">üß™ Usage</h2>

### LLM Module

This is a **Minimal Example** of what you can do with this package.  
Here you will find examples of how to run commands in terminal, trigger actions based on pattern matches, retrieve information from an endpoint, and more.  

> [!TIP]  
> When integrating this into your system, consider using the power of the LLM **only when truly necessary**.  
> In most cases, tasks can be solved with regular expressions, consuming information from a RAG, either running commands in terminal or calling an endpoint.

#### Agent Intents (`handle(data, tipo)`)
| `tipo`     | What it does | Input `data` | Output shape | What it represents |
|------------|--------------|---------------|--------------|--------------------|
| `rag`      | Returns `data` as-is (external RAG already resolved). | Pre-composed **string** from your RAG (e.g., `general_rag.json`) | `str` | Consult information from a RAG or dataset. |
| `general`  | Free-form Q&A via `llm.answer_general`. | Question | `str` | Minimal example of how to implement the LLM for general queries. |
| `battery`  | Reads battery percentage via `tool_get_batt()`. | Reads **Battery** status | `str` like `Mi bater√≠a es: 84.0%` (or a ‚Äúno reading‚Äù message) | Retrieve system information. |
| `maps`     | Reads maps via `tool_get_maps_from_backend()` and classifies between ‚Äúreturn maps‚Äù and ‚Äúthe number of maps‚Äù. | Reads **Maps** from an endpoint | Either the number of maps or the list of maps | Minimal example of consuming an API. |
| `navigate` | Navigates to a named place or generates a short motion. Attempts `tool_nav(data)` first (RAG/`poses.json`): if found, replies **"Voy"** (execute) or **"Por all√°"** (indicate/simulate). If not found, falls back to `llm.plan_motion(data)` ‚Üí `_clamp_motion(...)` ‚Üí `natural_move_llm(...)`. | Pre-composed string from your RAG (`poses.json`) or a natural-language command (e.g., `ve a la enfermer√≠a`, `gira 90¬∞ y avanza 0.5 m`). | Usually `str`. On fallback may return a **tuple**: `(mensaje, '{"yaw": <deg>, "distance": <m>}' )`. | Represents full integration: minimal example of running terminal commands to execute actions (`publish_natural_move()`). |

> If you consume the agent‚Äôs reply topic, handle both cases for `navigate`:  
> - Always log or speak the **text message** (the user-facing string).
> - Optionally process the JSON telemetry if it‚Äôs present.

> In other words, your subscriber must gracefully support both formats:
> - Sometimes the agent will only send a plain, human-readable string (e.g., ‚ÄúHeading to the kitchen.‚Äù).
> - Other times, it may include structured telemetry data (e.g., goal coordinates, ETA, path length).

---

#### Add a New Intent / Tool
The flow is: **patterns ‚Üí intent detection ‚Üí router ‚Üí tool implementation**.
Your text is normalized (`norm_text`) before matching (lowercase, accents removed, courtesy words stripped), so keep patterns as simple as possible.

---
##### 1) Declare patterns in `llm_patterns.py` 
Add a compiled regex that captures the trigger words for your new intent (example: ‚Äútime‚Äù intent).

```python
# llm_patterns.py
import re

TIME_WORDS_RE = re.compile(r"""
(?xi)
\b(
    hora|que\s+hora|time|current\s+time
)\b
""")
```
Add to the pattern executor
```python
# ______________________Cases that we want to execute_________________________________

#Here we define the functions, with the corresponding patterns, that we want to execute
INTENT_RES = {
    "battery":   BATTERY_WORDS_RE,
    "navigate":  MOV_VERB_RE,
    "time":      TIME_WORDS_RE,             #<- NEW
}

#Here we define the priority of the functions to be executed
INTENT_PRIORITY = ("time", "battery", "navigate") #<- NEW

# kind_group: "first" (short) == first or "second" (long) == second determine wich works are executed first
# need_user_input: True == needs the query to process the action, False == does not need it

INTENT_ROUTING = {
    "time":     {"kind_group": "first", "kind": "time",     "need_user_input": False},
    "battery":  {"kind_group": "first", "kind": "battery",  "need_user_input": False},
    "navigate": {"kind_group": "second", "kind": "navigate", "need_user_input": True},
}
```
---

##### 2) Implement the tool in `llm_tools.py` (class GetInfo)
Tools that should be spoken by **TTS should return a string**
(if you return a dict, your loop will JSON-dump it).

```python
def tool_get_time(self) -> str:
    from datetime import datetime
    now = datetime.now()
    return now.hour:02d, now.minute:02d 
```

- Add the new function to the `handle` in `llm_router.py`

---

##### 3) Wire it into the router
Pass the tool to the `Router` constructor and handle it in `llm_router.py`.

```python

# llm_router.py
class Router:
  . . .
  self.get_info = get_info
  self.handlers: Dict[str, Callable[[str], str]] = {
    "rag": self.data_return,
    "general": self.general_response_llm,
    "battery": self.battery_publisher,
    "navigate": self.navigation_publisher,
    "time": self.publish_time,             #<- NEW
  }
    . . .
    #------------ Handler or Router ------------------- 
    def handle(self, data: str, tipo: str) -> str:
       . . .

    #-------------The Publishers-------------------------
    def publish_time(self, data: str)-> str:                 #<- NEW
      hours, minutes = self.get_info.tool_get_time() 
      return f"Son las {hours}:{minutes}"

```
---
#### Avatar
Enable a simple on-screen ‚Äúavatar‚Äù to visualize the pipeline (wake-word ‚Üí STT ‚Üí LLM ‚Üí TTS).

##### 1) Turn it on

Open `config/settings.py` and set:

```python
AVATAR = True
```
##### 2) Give it access to your I/O
The avatar uses your microphone and speakers.
For setup notes (devices, permissions, troubleshooting), see the [**README.md**](avatar/README.md)

##### 3) Start the avatar
From your project root, either:

**Run the full pipeline** (recommended):
```python
python -m main
```
Or **run only the Wake Word** test (for a quick check):
```python
python -m stt.wake_word
```
---
##### What you should see
- **Idle Mode ‚Üí**  The ball (or badge) appears **blue** and remains static ‚Äî no deformation.
- **Wake-word detected ‚Üí** The ball turns **gold**, indicating it‚Äôs actively **listening**.
- **STT running ‚Üí** The ball **deforms dynamically**, following the **waveforms** of your voice input.
- **TTS speaking ‚Üí** The ball returns to **blue** and **moves** according to the **output audio waveform**.

> [!TIP]
> If the ball doesn‚Äôt deform, you‚Äôll see a message in the web interface.
> This usually means there‚Äôs a configuration issue ‚Äî please check the [**README.md**](avatar/README.md) for setup details.

This will open a local HTML page (the avatar UI). Try the full flow: say **‚Äúok robot, ¬øc√≥mo te llamas?‚Äù** and watch the indicators change.

Then run this to validate if the system works, an html file is going to be oppened and you migth see this. try to follow the full pipeline as ok robot como te llamas. You can see How your ball change of color when you saw ok robot and then is set again to the other color when it start to speek. 


<h2 id="contributing">ü§ù Contributing</h2>
Contributions are welcome! Please fork the repository and submit a pull request. For major changes, open an issue first to discuss what you would like to change.

1. Fork the repository.
2. Create a new branch (`git checkout -b feature/your-feature-name`).
3. Make your changes.
4. Commit your changes (`git commit -m 'Add some amazing feature'`).
5. Push to the branch (`git push origin feature/your-feature-name`).
6. Open a Pull Request.

---

<h2 id="license">üìÑ License</h2>
This project is licensed under the [MIT License](LICENSE).

---

<h2 id="acknowledgements">üôè Acknowledgements</h2>

- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Vosk](https://alphacephei.com/vosk/)
- [Qwen Models](https://huggingface.co/Qwen)
- The ROS2 community
